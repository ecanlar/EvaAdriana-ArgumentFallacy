{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c78b8eae",
   "metadata": {},
   "source": [
    "# DeepSeek fallacy classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "613be702",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a81e531",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EvalPrediction\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    f1_macro = f1_score(labels, predictions, average=\"macro\")\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions, average='weighted')\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1_macro\": f1_macro,\n",
    "        \"f1\": f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61cc8b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e7738d1411e436f93e2275e468c4c2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ModifiedModelForMulticlassClassification(\n",
       "  (transformer): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "# Configuración de cuantización 4-bit\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",  # NF4 recomendado para QLoRA\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True  # Opcional: nested quantization\n",
    ")\n",
    "\n",
    "# Cargar el modelo original como AutoModelForCausalLM\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "# Asignamos un pad token para que el modelo pueda manejar batch_size > 1\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Cargar el modelo de lenguaje causal\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "class ModifiedModelForMulticlassClassification(nn.Module):\n",
    "    def __init__(self, original_model, num_classes=6):\n",
    "        super().__init__()\n",
    "        self.transformer = original_model.model\n",
    "        self.lm_head = nn.Linear(self.transformer.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        outputs = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "\n",
    "        # Pooling: media a lo largo de la secuencia\n",
    "        pooled_output = hidden_states.mean(dim=1)\n",
    "\n",
    "        # Asegurar que el dtype coincida con el de lm_head\n",
    "        pooled_output = pooled_output.to(dtype=self.lm_head.weight.dtype)\n",
    "\n",
    "        logits = self.lm_head(pooled_output)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.lm_head.out_features), labels.view(-1))\n",
    "            return loss, logits\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "# Reemplazar el modelo original con el modelo modificado\n",
    "model = ModifiedModelForMulticlassClassification(model, num_classes=6)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c895af9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c06fba24abe4eccbf878ee156517db1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/982 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7946cda546ec4a41a27dedeff7ba205a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/246 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Cargar el tokenizer y asignar el pad_token\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Asignar el pad_token como el eos_token\n",
    "\n",
    "df = pd.read_csv(\"data/train_afc.csv\")\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, stratify=df[\"Etiqueta\"], random_state=42)\n",
    "train_df = train_df.rename(columns={\"Etiqueta\": \"labels\"})\n",
    "val_df = val_df.rename(columns={\"Etiqueta\": \"labels\"})\n",
    "\n",
    "# Crear la función de tokenización con el prompt\n",
    "def tokenize_function(examples):\n",
    "    prompt = \"\"\"\n",
    "    Your task is to classify a fallacy in the given text into one of the following categories (numerical labels):\n",
    "    \n",
    "    0: Appeal to Emotion\n",
    "    \n",
    "    1: Appeal to Authority\n",
    "\n",
    "    2: Ad Hominem\n",
    "\n",
    "    3: False Cause\n",
    "\n",
    "    4: Slippery Slope\n",
    "\n",
    "    5: Slogans\n",
    "    \n",
    "    Task: Classify the following text snippet into one of the fallacy categories (0-5). The text is:\n",
    "    \"\"\"\n",
    "    \n",
    "    # Concatenate the prompt with the dataset text\n",
    "    text_with_prompt = [prompt + text for text in examples[\"Texto\"]]\n",
    "    return tokenizer(text_with_prompt, padding=\"max_length\", truncation=True, max_length=256)\n",
    "\n",
    "\n",
    "# Tokenizar los datos\n",
    "train_dataset = Dataset.from_pandas(train_df).map(tokenize_function, batched=True)\n",
    "val_dataset = Dataset.from_pandas(val_df).map(tokenize_function, batched=True)\n",
    "\n",
    "# Establecer el formato adecuado para el dataset\n",
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "31207cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencia: 100%|██████████| 31/31 [00:37<00:00,  1.19s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm  # Asegúrate de importar esto al principio de tu código\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Establecer el modelo en modo evaluación\n",
    "model.eval()\n",
    "\n",
    "# Crear un DataLoader para la inferencia\n",
    "dataloader = DataLoader(val_dataset, batch_size=8)\n",
    "\n",
    "predictions = []\n",
    "\n",
    "# Barra de progreso con tqdm\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(dataloader, desc=\"Inferencia\", total=len(dataloader)):  # Aquí va la barra\n",
    "        # Mover tensores al dispositivo (GPU o CPU)\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "        # Asegurarse de que los input_ids sean del tipo correcto (Long) y no float16\n",
    "        input_ids = input_ids.to(dtype=torch.long)  # Convertir input_ids a Long\n",
    "        attention_mask = attention_mask.to(dtype=torch.bfloat16)  # Mantener la máscara como float16 si es necesario\n",
    "\n",
    "        # Realizar inferencia\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs\n",
    "\n",
    "        # Obtener las predicciones\n",
    "        preds = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "        predictions.extend(preds)\n",
    "\n",
    "# Guardar las predicciones en el dataframe\n",
    "val_df['predicted_label'] = predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "58394960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2236\n",
      "F1 Score: 0.2312\n",
      "F1 Macro Score: 0.0921\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Calcula accuracy y F1 score\n",
    "accuracy = accuracy_score(val_df['labels'], val_df['predicted_label'])\n",
    "f1 = f1_score(val_df['labels'], val_df['predicted_label'], average='weighted')\n",
    "f1_macro = f1_score(val_df['labels'], val_df['predicted_label'], average='macro')\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"F1 Macro Score: {f1_macro:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e251b2f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca13934574cb443c966818a6bec86e2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1228 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "# Concatenar los datasets de entrenamiento y validación\n",
    "full_train_df = concatenate_datasets([train_dataset, val_dataset]).map(tokenize_function, batched=True)\n",
    "full_train_df.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c38509de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66745d02f120441fb4273c67722187b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2160 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencia test: 100%|██████████| 270/270 [05:27<00:00,  1.21s/it]\n"
     ]
    }
   ],
   "source": [
    "# Cargar test\n",
    "test_df = pd.read_csv(\"data/test_afc.csv\")\n",
    "test_df = test_df.rename(columns={\"Etiqueta\": \"labels\"})\n",
    "\n",
    "# Tokenizar igual que los demás\n",
    "test_dataset = Dataset.from_pandas(test_df).map(tokenize_function, batched=True)\n",
    "test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# Crear DataLoader\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8)\n",
    "\n",
    "# Inference\n",
    "model.eval()\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_dataloader, desc=\"Inferencia test\", total=len(test_dataloader)):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "        input_ids = input_ids.to(dtype=torch.long)\n",
    "        attention_mask = attention_mask.to(dtype=torch.bfloat16)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs\n",
    "\n",
    "        preds = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "        predictions.extend(preds)\n",
    "\n",
    "# Guardar resultados\n",
    "test_df[\"predicted_label\"] = predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1976bb7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predicted_label\n",
       "2    2029\n",
       "0     115\n",
       "1      16\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.predicted_label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "de0fb06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv(\"afc_deepseak_text.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35e01b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "haia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
